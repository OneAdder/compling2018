{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dostoevsky = open('besy_dostoevsky.short').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    normalized_text = [\n",
    "        word.strip(punctuation) for word in text.lower().split()\n",
    "    ]\n",
    "    normalized_text = [word for word in normalized_text if word]\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrammer(tokens, n):\n",
    "    ngrams = []\n",
    "    for i in range(0, len(tokens) - n+1):\n",
    "        ngrams.append(' '.join(tokens[i: i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_dostoevsky = [['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(dostoevsky)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_dostoevsky = Counter()\n",
    "bigrams_dostoevsky = Counter()\n",
    "trigrams_dostoevsky = Counter()\n",
    "\n",
    "for sentence in sentences_dostoevsky:\n",
    "    unigrams_dostoevsky.update(sentence)\n",
    "    bigrams_dostoevsky.update(ngrammer(sentence, 2))\n",
    "    trigrams_dostoevsky.update(ngrammer(sentence, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<start> — я', 133),\n",
       " ('<start> — да', 77),\n",
       " ('<start> — а', 69),\n",
       " ('<start> — вы', 66),\n",
       " ('<start> варвара петровна', 53),\n",
       " ('<start> — это', 53),\n",
       " ('<start> — не', 52),\n",
       " ('<start> николай всеволодович', 50),\n",
       " ('<start> степан трофимович', 47),\n",
       " ('по крайней мере', 42)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams_dostoevsky.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<start> —', 1930),\n",
       " ('<start> я', 283),\n",
       " ('варвара петровна', 211),\n",
       " ('николай всеволодович', 198),\n",
       " ('<start> он', 187),\n",
       " ('— я', 184),\n",
       " ('и не', 182),\n",
       " ('<start> но', 181),\n",
       " ('степан трофимович', 180),\n",
       " ('что я', 147)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_dostoevsky.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_dostoevsky = np.zeros((\n",
    "    len(bigrams_dostoevsky),\n",
    "    len(unigrams_dostoevsky),\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word_dostoevsky = list(unigrams_dostoevsky)\n",
    "id2word_bigrams_dostoevsky = list(bigrams_dostoevsky)\n",
    "\n",
    "word2id_dostoevsky = {word: i for i, word in enumerate(id2word_dostoevsky)}\n",
    "word2id_bigrams_dostoevsky = {word: i for i, word in enumerate(id2word_bigrams_dostoevsky)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ngram in trigrams_dostoevsky:\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    bigram = '{} {}'.format(word1, word2)\n",
    "    matrix_dostoevsky[\n",
    "        word2id_bigrams_dostoevsky[bigram]\n",
    "    ][\n",
    "        word2id_dostoevsky[word3]\n",
    "    ] = (\n",
    "        trigrams_dostoevsky[ngram] / bigrams_dostoevsky[bigram]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ремарка\n",
    "Стоит сказать, что можно было придумать что-то умнее с тегом < start >, но не очень понятно что.\n",
    "Я попытался заставить выбирать какую-то другую биграмму, если в ней был этот тег. Однако, в таком случае, модель продолжает пихать мне эту же самую биграмму."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(matrix, id2word, word2id, n=100):\n",
    "    starts = [bigr for bigr in list(word2id) if bigr.startswith('<start>')]\n",
    "    start = np.random.choice(starts)\n",
    "    text = []\n",
    "    current_idx = word2id[start]\n",
    "    for i in range(n):\n",
    "        if sum(matrix[current_idx]):\n",
    "            chosen =\\\n",
    "                np.random.choice(\n",
    "                    matrix.shape[1],\n",
    "                    p=matrix[current_idx],\n",
    "                )\n",
    "            text.append(id2word[chosen])\n",
    "        if id2word[chosen].endswith('<end>'):\n",
    "            chosen = word2id[np.random.choice(starts)]\n",
    "        current_idx = chosen\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "родного сиволапого\n",
      "так пришедши к рассказали в и потонуло\n",
      "он как от многоуважаемой варваре припомню пропел подробностей из в здравом жители смотреть крутизны в и бросилось стадо свиней и они просили его чтобы позволил им войти в них уничтожении наследства постороннему лицу степане трофимовиче трофимовича верховенского\n",
      "видевшие и пришедши и рассказали в них которым очень конец его для самолюбия боже тем которую я прочла бессмертную фактов явились и они многочтимом степане степана трофимовича\n",
      "он потом постепенно в них нет\n",
      " лучше сказать трофимович постоянно по неумению\n",
      "часть эту картинку в них которым очень конец его для самолюбия боже тем которую я прочла бессмертную фактов явились и они многочтимом степане степана трофимовича послужат лишь недавних и им как к иисусу в городе вечера стояли не омраченной в петербурге привычки или степан трофимович страшное будет\n",
      "он луки\n",
      " степане трофимовиче трофимовича верховенского\n",
      "видевшие и пришедши и рассказали в них о талантливом и они многочтимом степане степана трофимовича ему вовсе что под приятного для сохрани боже история которую глава viii бесы сидящего по деревням\n",
      "он публики знать луки\n",
      " ее напечатать обоих словечках даже что они просили его чтобы позволил им войти в них\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    generate(\n",
    "        matrix_dostoevsky,\n",
    "        id2word_bigrams_dostoevsky,\n",
    "        word2id_bigrams_dostoevsky,\n",
    "    ).replace(' <end>', '\\n').replace(' <start> ', '\\n')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "from gensim.models.phrases import Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('anna_karenina.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['И вот возьми телеграмму, передай, что они скажут.',\n",
       " '«Попробовать хотите», – понял Матвей, но он сказал только:\\n\\n– Слушаю-с.',\n",
       " 'Степан Аркадьич уже был умыт и расчесан и сбирался одеваться, когда Матвей, медленно ступая поскрипывающими сапогами, с телеграммой в руке, вернулся в комнату.',\n",
       " 'Цирюльника уже не было.',\n",
       " '– Дарья Александровна приказали доложить, что они уезжают.',\n",
       " 'Пускай делают, как им, вам то есть, угодно, – сказал он, смеясь только глазами, и, положив руки в карманы и склонив голову набок, уставился на барина.',\n",
       " 'Степан Аркадьич помолчал.',\n",
       " 'Потом добрая и несколько жалкая улыбка показалась на его красивом лице.',\n",
       " '– А?',\n",
       " 'Матвей?']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = sent_tokenize(text)\n",
    "sents[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = [list(sent) for sent in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases1 = Phrases(symbols, scoring='npmi', threshold=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases2 = Phrases(phrases1[symbols], scoring='npmi', threshold=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases3 = Phrases(phrases2[phrases1[symbols]], scoring='npmi', threshold=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases4 = Phrases(phrases3[phrases2[phrases1[symbols]]], scoring='npmi', threshold=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases5 = Phrases(phrases4[phrases3[phrases2[phrases1[symbols]]]], scoring='npmi', threshold=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = Phrases(phrases5[phrases4[phrases3[phrases2[phrases1[symbols]]]]], scoring='npmi', threshold=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmn = Counter(phrases.vocab).most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "?\n",
      ",_ _и_ \n",
      "е_г_о_ \n",
      " _н_е_ \n",
      "е_.\n",
      "!\n",
      "а_.\n",
      " _б_ы_л\n",
      " _э_т_о\n",
      " _н_а_ \n",
      " _е_г_о\n",
      " _о_н_ \n",
      "О_н_а_ \n",
      " _п_р_и\n"
     ]
    }
   ],
   "source": [
    "for t in cmn:\n",
    "    print(t[0].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_adapt_by_suffix',\n",
       " '_apply',\n",
       " '_load_specials',\n",
       " '_save_specials',\n",
       " '_smart_save',\n",
       " 'add_vocab',\n",
       " 'analyze_sentence',\n",
       " 'common_terms',\n",
       " 'corpus_word_count',\n",
       " 'delimiter',\n",
       " 'export_phrases',\n",
       " 'learn_vocab',\n",
       " 'load',\n",
       " 'max_vocab_size',\n",
       " 'min_count',\n",
       " 'min_reduce',\n",
       " 'progress_per',\n",
       " 'save',\n",
       " 'score_item',\n",
       " 'scoring',\n",
       " 'threshold',\n",
       " 'vocab']"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Annotation', '«Анна', 'Каренина', ' это', 'сложное', 'психологически', 'утонченное', ' остропроблемное', 'произведение', ' насыщенное', 'приметами', 'времени', ' Л', '.Н', ' Толстой', 'на', 'страницах', 'произведения', 'показывает', 'как', 'рушатся']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    tokens = []\n",
    "    tmp = ''\n",
    "    for bigram in phrases[text]:\n",
    "        if bigram[0] in punctuation + ' \\n«»–':\n",
    "            if not tmp in ' \\n':\n",
    "                tokens.append(tmp)\n",
    "            tmp = bigram[-1]\n",
    "        elif bigram[-1] in punctuation + ' \\n«»–':\n",
    "            if not tmp in ' \\n':\n",
    "                tokens.append(tmp + bigram[0])\n",
    "            tmp=''\n",
    "        else:\n",
    "            tmp += bigram.replace('_', '')\n",
    "    return tokens\n",
    "\n",
    "t = text[:200]\n",
    "print(tokenize(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Итог\n",
    "Возможно, если получше написать обёртку, будет работать хорошо."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
